{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005a2b86",
   "metadata": {},
   "source": [
    "# Single View of Customer\n",
    "\n",
    "The `svoc` package performs record linkage between two dataframes.  \n",
    "The objective is to link each record from a _benchmark dataframe_ to at least one record from a second dataframe, referred to as the _input dataframe_.\n",
    "\n",
    "The script takes as input:\n",
    "- Two dataframes, benchmark and input (see [Data](#data));\n",
    "- A [configuration file](#configuration-file), either in `.yaml` or `.env` format.\n",
    "\n",
    "The record linkage process consists of four main steps:\n",
    "\n",
    "1. [Data preparation](#data-preparation)\n",
    "2. [Features calculation](#features-calculation)\n",
    "3. [Automatic matching](#automatic-matching)\n",
    "4. [Supervised matching](#supervised-matching)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97b74cb",
   "metadata": {},
   "source": [
    "\n",
    "## Data\n",
    "\n",
    "Record linkage is performed by comparing the similarity of fields across dataframes.  \n",
    "The current implementation requires the data to contain the following fields:\n",
    "\n",
    "| Field        | Content                                      | Example                              |\n",
    "|:-------------|:---------------------------------------------|:-------------------------------------|\n",
    "| ID           | Row ID that uniquely identifies each record  | c9425868-0cd2-48eb-9050-8431d9832838 |\n",
    "| OUTLET_NAME  | Name of the outlet                           | The Golden Fleece                    |\n",
    "| ADDRESS      | Address of the outlet                        | 9 Queen St, London                   |\n",
    "| POSTCODE     | Postal code of the outlet                    | EC4N1SP                              |\n",
    "\n",
    "The data may use different column names; in that case, the corresponding column mappings must be specified in the [configuration file](#configuration-file).\n",
    "\n",
    "## Configuration file\n",
    "\n",
    "The `/config` folder contains the .yaml configuration file.  \n",
    "In the configuration file, you can set the following parameters:\n",
    "\n",
    "| Parameter               | Description                                                                 | Default value                               | Example                                   |\n",
    "|:------------------------|:-----------------------------------------------------------------------------|:--------------------------------------------|:------------------------------------------|\n",
    "| DATA_DIR                | Directory where the benchmark and input `.csv` files are stored              | `./` (current directory)                    | `./data`                                  |\n",
    "| BENCHMARK_DATA_FILENAME | Name of the `.csv` file containing the benchmark data                         | —                                          | `benchmark_data.csv`                      |\n",
    "| INPUT_DATA_FILENAME     | Name of the `.csv` file containing the input data                             | —                                          | `input_data.csv`                          |\n",
    "| BENCHMARK_DATATABLE     | Name of the SQL table containing the benchmark data                           | —                                          | `data.benchmarkdata`                      |\n",
    "| INPUT_DATATABLE         | Name of the SQL table containing the input data                               | —                                          | `data.inputdatatable`                    |\n",
    "| BENCHMARK_COLUMNS       | Mapping of required fields to benchmark data column names                     | `{ID, OUTLET_NAME, ADDRESS, POSTCODE}`     | see example below                         |\n",
    "| INPUT_COLUMNS           | Mapping of required fields to input data column names                         | `{ID, OUTLET_NAME, ADDRESS, POSTCODE}`     | see example below                         |\n",
    "| MODELS_DIR              | Directory where supervised models are stored                                  | `./models`                                 | `./new_models`                            |\n",
    "| N_MATCHES               | Maximum number of matches (from input data) per benchmark record              | `3`                                        | `1`                                      |\n",
    "| BLOCK_COL               | Column used for blocking (matches must share the same value)                  | `'POSTCODE'`                               | _Other values not currently supported_   |\n",
    "\n",
    "#### _Example_\n",
    "\n",
    "We want to find a match for each record in the SAP data stored in the CSV file `./data/HUK_sap_data.csv`.  \n",
    "Possible matches are searched within the Bowimi data stored in `./data/HUK_bowimi_data.csv`.\n",
    "\n",
    "Since the column names differ from the default ones (see the table above or the [Data](#data) section), they must be explicitly mapped in the configuration file:\n",
    "\n",
    "```yaml\n",
    "DATA_DIR: \"./data\"\n",
    "\n",
    "BENCHMARK_DATA_FILENAME: \"HUK_sap_data.csv\"\n",
    "BENCHMARK_COLUMNS:\n",
    "  ID: \"SapCode\"\n",
    "  OUTLET_NAME: \"OutletName\"\n",
    "  POSTCODE: \"OutletPostcode\"\n",
    "  ADDRESS: \"OutletAddress\"\n",
    "\n",
    "INPUT_DATA_FILENAME: \"HUK_bowimi_data.csv\"\n",
    "INPUT_COLUMNS:\n",
    "  ID: \"BowimiId\"\n",
    "  OUTLET_NAME: \"OutletName\"\n",
    "  POSTCODE: \"OutletPostCode\"\n",
    "  ADDRESS: \"OutletAddress\"\n",
    "\n",
    "```\n",
    "If the data are stored in SQL tables, they can be imported by specifying the corresponding `*_DATATABLE` parameters instead of the `*_FILENAME` ones.\n",
    "```yaml\n",
    "BENCHMARK_FILENAME: \"rl_data.huk_sap_table\"\n",
    "BENCHMARK_COLUMNS:\n",
    "  ID: 'SapCode'\n",
    "  OUTLET_NAME: 'OutletName'\n",
    "  POSTCODE: 'OutletPostcode'\n",
    "  ADDRESS: 'OutletAddress'\n",
    "\n",
    "INPUT_FILENAME: \"rl_data.huk_bowimi_table\"\n",
    "INPUT_COLUMNS:\n",
    "  ID: 'BowimiId'\n",
    "  OUTLET_NAME: 'OutletName'\n",
    "  POSTCODE: 'OutletPostCode'\n",
    "  ADDRESS: 'OutletAddress'\n",
    "```\n",
    "\n",
    "If a parameter is not specified in the configuration file, the default value will be used.\n",
    "\n",
    "### Environment variables\n",
    "\n",
    "Instead of using a `.yaml` file, parameters can be set as environment variables in a `.env` file.  \n",
    "In this case:\n",
    "\n",
    "- Parameter names must be prefixed with `SVOC_`;\n",
    "- Nested parameters must be specified using a double underscore (`__`) as a separator.\n",
    "\n",
    "#### _Example_\n",
    "\n",
    "```env\n",
    "SVOC_INPUT_FILENAME=\"rl_data.huk_bowimi_table\"\n",
    "SVOC_BENCHMARK_COLUMNS__ID=\"SapCode\"\n",
    "SVOC_BENCHMARK_COLUMNS__OUTLET_NAME=\"OutletName\"\n",
    "# ...\n",
    "```\n",
    "\n",
    "### Importing the settings\n",
    "\n",
    "After defining the parameters, they can be loaded into the script using \n",
    "```python \n",
    "from svoc.settings import get_settings()\n",
    "settings = get_settings()   # Load from .env\n",
    "# or\n",
    "settings = get_settings(\"./config/settings.yaml\") # Load from a .yaml file \n",
    "``` \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfefd0e",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "The first step of the algorithm consists of a data preparation phase, whose goal is to harmonize and clean the input datasets before performing the record linkage.\n",
    "\n",
    "#### Data loading and schema alignment\n",
    "\n",
    "As a first step, the input datasets are imported and only the columns of interest are retained.These columns are defined in the [configuration file](#configuration-file).\n",
    "\n",
    "To ensure consistency between the two data sources, the selected columns are then renamed so that field names are aligned across both dataframes. This step guarantees that equivalent information (e.g. outlet name, address) is represented using a common schema.\n",
    "\n",
    "#### Case normalization\n",
    "\n",
    "Since the matching process is case sensitive, all string fields are converted to uppercase.\n",
    "This normalization step avoids mismatches caused solely by differences in capitalization (e.g. Street vs STREET).\n",
    "\n",
    "#### Removal of special characters\n",
    "\n",
    "All strings are then cleaned by removing special characters and symbols.\n",
    "Only letters, numbers, and spaces are preserved. This step reduces noise introduced by punctuation, formatting differences, or non-standard characters that are not informative for matching purposes.\n",
    "\n",
    "#### Creation of standardized clean fields\n",
    "\n",
    "In addition to the original fields, two new columns are created:\n",
    "\n",
    "- ADDRESS_CLEAN\n",
    "- OUTLET_NAME_CLEAN\n",
    "\n",
    "These columns are derived from ADDRESS and OUTLET_NAME respectively and are specifically designed to improve matching quality.\n",
    "\n",
    "For addresses, a standardization process is applied to harmonize common variations.\n",
    "In particular, frequently used abbreviations are expanded to their full form (e.g. RD → ROAD, LN → LANE, ST → STREET). This reduces inconsistencies caused by alternative address representations.\n",
    "\n",
    "For outlet names, instead, a filtering approach is adopted.\n",
    "Common and non-informative words (such as THE, BAR, PUB, LTD, etc.) are removed, as they tend to appear frequently across different records and may negatively affect the matching process without adding discriminative power.\n",
    "\n",
    "#### Final dataset structure\n",
    "\n",
    "At the end of the data preparation phase, both dataframes retain:\n",
    "\n",
    "- all the original columns of interest\n",
    "- the corresponding standardized CLEAN versions\n",
    "\n",
    "The subsequent matching step will leverage both the original and cleaned fields, allowing the algorithm to balance strict matching with more robust, noise-resistant comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d2441",
   "metadata": {},
   "source": [
    "## Features Calculation\n",
    "\n",
    "Record linkage is performed by comparing the fields of two different dataframes and matching those records whose field values are most similar.\n",
    "\n",
    "To assess the similarity between two fields, the algorithm relies on the Python\n",
    "[recordlinkage](https://recordlinkage.readthedocs.io/en/latest/) package, which provides classes and methods to compute several distance measures, referred to as _features_.\n",
    "\n",
    "The similarity between two strings is represented by a numerical value between 0 (no similarity) and 1 (maximum similarity).  \n",
    "The `recordlinkage` package offers multiple string distance metrics; the `svoc` package uses the following:\n",
    "\n",
    "- _Jaro–Winkler_ distance;\n",
    "- _Levenshtein_ distance;\n",
    "- _Q-gram_ distance;\n",
    "- _Cosine_ distance.\n",
    "\n",
    "In addition, the package includes the following custom measures:\n",
    "\n",
    "- _Exact_: equal to 1 if the strings are exactly the same, 0 otherwise;\n",
    "- _Substring_: equal to 1 if one string is entirely contained within the other, 0 otherwise;\n",
    "- _Word inclusion_: equal to 1 if all the words in one string are contained in the other, 0 otherwise.\n",
    "\n",
    "All available measures are defined in the `DISTANCES` constant:\n",
    "\n",
    "```python\n",
    "from svoc.constants import DISTANCES\n",
    "DISTANCES\n",
    "```\n",
    "This is a list of `Distance()` instances, defined as follows:\n",
    "``` python\n",
    "from svoc.automatic.enums import DistanceMethod\n",
    "from svoc.automatic.models import Distance\n",
    "Distance(\n",
    "    col_name = 'OUTLET_NAME',          # Field name\n",
    "    method = DistanceMethod.COSINE,    # Distance metric used\n",
    "    label = 'outlet_name_cosine'       # Feature label\n",
    ")\n",
    "```\n",
    "These measures are computed for the pairs of records that share the value of the `BLOCK_COL` field (set in the [configuration file](#configuration-file)). With the current implementation, the pairs of records have the same postal code. This constraint guarantees less computational costs and more precision. \n",
    "\n",
    "The following table contains the features currently calculated.\n",
    "\n",
    "| Field Name        | Method          | Label                                |\n",
    "|:------------------|:----------------|:-------------------------------------|\n",
    "| OUTLET_NAME       | Cosine          | outlet_name_cosine                   |\n",
    "|                   | Jarowinkler     | outlet_name_jarowinkler              |\n",
    "|                   | Levenshtein     | outlet_name_lenvenshtein             |\n",
    "|                   | QGram           | outlet_name_qgram                    |\n",
    "|                   | Exact           | outlet_name                          |\n",
    "|                   | Substring       | outlet_name_in                       |\n",
    "|                   | Word Inclusion  | outlet_name_in2                      |\n",
    "| OUTLET_NAME_CLEAN | Cosine          | outlet_name_clean_cosine             |\n",
    "|                   | Jarowinkler     | outlet_name_clean_jarowinkler        |\n",
    "|                   | Levenshtein     | outlet_name_clean_lenvenshtein       |\n",
    "|                   | QGram           | outlet_name_clean_qgram              |\n",
    "|                   | Exact           | outlet_name_clean                    |\n",
    "|                   | Substring       | outlet_name_clean_in                 |\n",
    "|                   | Word Inclusion  | outlet_name_clean_in2                |\n",
    "| ADDRESS           | Cosine          | address_cosine                       |\n",
    "|                   | Jarowinkler     | address_jarowinkler                  |\n",
    "|                   | Levenshtein     | address_lenvenshtein                 |\n",
    "|                   | QGram           | address_qgram                        |\n",
    "|                   | Exact           | address                              |\n",
    "|                   | Substring       | address_in                           |\n",
    "|                   | Word Inclusion  | address_in2                          |\n",
    "| ADDRESS_CLEAN     | Cosine          | address_clean_cosine                 |\n",
    "|                   | Jarowinkler     | address_clean_jarowinkler            |\n",
    "|                   | Levenshtein     | address_clean_lenvenshtein           |\n",
    "|                   | QGram           | address_clean_qgram                  |\n",
    "|                   | Exact           | address_clean                        |\n",
    "|                   | Substring       | address_clean_in                     |\n",
    "|                   | Word Inclusion  | address_clean_in2                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df87b20e",
   "metadata": {},
   "source": [
    "## Automatic Matching\n",
    "\n",
    "The automatic matching consists in selecting the matching pairs of records through a sequence of **filtering steps**.  The filters, which are progressively less restrictive, apply constraints to specific similarity measures. Through these filters, for each record from the benchmark dataset the algorithm selects up to 3 matching records from the input dataframe. The maximum number of matches is set through the `N_MATCHES` parameter in the [configuration file](#configuration-file).\n",
    "\n",
    "### Filters \n",
    "\n",
    "You can print the list of filters with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "263ca2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[DistanceFilter(value={'outlet_name': 0.5, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_clean_cosine': 0.7}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_clean_levenshtein': 0.7}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_clean_qgram': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_clean_cosine': 0.7, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean_levenshtein': 0.7, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean_qgram': 0.65, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_cosine': 0.7, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_levenshtein': 0.7, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_qgram': 0.65, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_cosine': 0.7}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_levenshtein': 0.7}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_qgram': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_in': 0.5, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean_in': 0.5, 'address_clean': 0.5}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_in': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean': 0.5, 'address_clean_in': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_in': 0.5, 'address_clean_in': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_in': 0.5, 'address_in': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_in2': 0.5, 'address_in2': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean_in2': 0.5, 'address_clean_in2': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean_cosine': 0.85, 'address_clean_cosine': 0.85}),\n",
       " DistanceFilter(value={'outlet_name_clean_levenshtein': 0.8, 'address_clean_levenshtein': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_clean_jarowinkler': 0.8, 'address_clean_jarowinkler': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_clean_qgram': 0.65, 'address_clean_qgram': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_cosine': 0.8, 'address_cosine': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_levenshtein': 0.8, 'address_levenshtein': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_jarowinkler': 0.8, 'address_jarowinkler': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_qgram': 0.65, 'address_qgram': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_cosine': 0.8, 'address_clean_cosine': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_levenshtein': 0.8, 'address_clean_levenshtein': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_jarowinkler': 0.8, 'address_clean_jarowinkler': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_qgram': 0.65, 'address_clean_qgram': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_clean_cosine': 0.8, 'address_cosine': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_clean_levenshtein': 0.8, 'address_levenshtein': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_clean_jarowinkler': 0.8, 'address_jarowinkler': 0.8}),\n",
       " DistanceFilter(value={'outlet_name_clean_qgram': 0.65, 'address_qgram': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_clean_jarowinkler': 0.65, 'address_clean_levenshtein': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_clean_levenshtein': 0.65, 'address_clean_cosine': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_clean_jarowinkler': 0.65, 'address_clean_cosine': 0.65}),\n",
       " DistanceFilter(value={'outlet_name_clean_jarowinkler': 0.6, 'address_clean_jarowinkler': 0.6}),\n",
       " DistanceFilter(value={'outlet_name': 0.5, 'address_cosine': 0.0}),\n",
       " DistanceFilter(value={'outlet_name_cosine': 0.0, 'address': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean': 0.5, 'address_cosine': 0.0}),\n",
       " DistanceFilter(value={'outlet_name_cosine': 0.0, 'address_clean': 0.5}),\n",
       " DistanceFilter(value={'outlet_name_clean_in2': 0.5, 'address_cosine': 0.0}),\n",
       " DistanceFilter(value={'outlet_name_cosine': 0.0, 'address_clean_in2': 0.5}),\n",
       " DistanceFilter(value={'address_clean_cosine': 0.8, 'outlet_name_clean_cosine': 0.3}),\n",
       " DistanceFilter(value={'address_clean_levenshtein': 0.8, 'outlet_name_clean_cosine': 0.3}),\n",
       " DistanceFilter(value={'address_clean_jarowinkler': 0.8, 'outlet_name_clean_cosine': 0.3}),\n",
       " DistanceFilter(value={'address_clean_levenshtein': 0.8, 'outlet_name_clean_levenshtein': 0.2})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from svoc.constants import FILTERS_AUTO\n",
    "FILTERS_AUTO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a09208",
   "metadata": {},
   "source": [
    "\n",
    "This is a list of `DistanceFilter()` istances, defined as follows:\n",
    "\n",
    "```python\n",
    "from svoc.contants import DistanceFilter\n",
    "filter = DistanceFilter(\n",
    "    value = {\n",
    "        'outlet_name_cosine': 0.8,\n",
    "        'address_levenshtein': 0.7\n",
    "        }\n",
    "    )\n",
    "```\n",
    "\n",
    "The `value` parameter is a dictionary where each key is chosen among the labels of the `Distance()` instances (see [Features](#features-calculation)) and the associated value is the minimum value for the corresponding measure allowed by the filter. In the example above, the filter select as matching pairs all those records whose Cosine distance between the outlet names is al least equal to 0.8 *and* the levensthein distance between the addresses is at least 0.7. \n",
    "\n",
    "Each matching couple will be associated to a overall similarity score given by the aritmetic mean of the similarity measures considered by the filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c690169",
   "metadata": {},
   "source": [
    "## Supervised Matching\n",
    "\n",
    "The supervised matching is a probabilistic matching method that follows the automatic one. The aim of this step is to find a match for those records from the benchmark datased which remain un-matched after the filtering process. Records for which the previous step found fewer than 3 matches (`N_MATCHES`) are also included.\n",
    "\n",
    "The supervised matching exploits some trained models which are probabilistic decision rules that estimate how likely is that two records matches. \n",
    "\n",
    "The models are trained during the algorithm development and saved in different pickle files. \n",
    "The folder path where to find the models pickle files can ben set with the `MODELS_DIR` parameter in the [configuration file](#configuration-file).\n",
    "\n",
    "In the current implementation, three supervised models have been trained (see [recordlinkage documentation](https://recordlinkage.readthedocs.io/en/latest/guides/classifiers.html#supervised-learning) for details):\n",
    "\n",
    "1. Logistic Regression;\n",
    "2. Support Vector Machine (SVM);\n",
    "3. Naive Bayes Classifier.\n",
    "\n",
    "The logistic model and the bayes classifier provide a score which is the probability of matching for each pair of record: the pair is a match if the score is greater than 0.5. The SVM does not provide a probability but simply differentiates between matches and non-matches. The score for these matches is set equal to 0.5 by default.\n",
    "\n",
    "### Models Training\n",
    "\n",
    "The supervised models are **already trained** and provided out of the box with the current implementation.\n",
    "However, the training procedure can be re-executed if needed (e.g. to change features, distance metrics, or to retrain the models on a different labeled dataset).\n",
    "\n",
    "The supervised models are trained using a subset of records from the input dataset for which the correct match with the benchmark dataset is known.\n",
    "\n",
    "The benchmark and the input datasets contain their unique identifier column (ID).\n",
    "To train the models, the input dataset also have to contain an additional column that specifies the ID of the matching record in the benchmark dataset (e.g. benchmark_id). Only the input records for which this matching ID is available are used for training.\n",
    "This reference column is specified via the `input_cols_id_benchmark` parameter in the `train_all_model()`.\n",
    "Not all input records are required to have a match:\n",
    "- records where `input_cols_id_benchmark` is NaN are automatically excluded from training\n",
    "- only records with a known and valid benchmark match are used to build the training set.\n",
    "\n",
    "#### Feature Generation and Filtering\n",
    "\n",
    "During the model training:\n",
    "1. Similarity features are computed between all candidate pairs of records;\n",
    "2. Only benchmark records that appear at least once in the known matches are retained.\n",
    "\n",
    "The final training set consists of:\n",
    "- feature vectors for candidate pairs\n",
    "- a label derived from whether the pair corresponds to a known match\n",
    "\n",
    "#### Model Re-training\n",
    "\n",
    "Models can be (re)estimated using the `train_all_models()` function from the\n",
    "`svoc.supervised.match` module. Trained models are saved to the directory specified in path_models.\n",
    "\n",
    "**Important**: if the features used for record linkage change (e.g. input columns, benchmark columns, or distance metrics), all supervised models must be retrained.\n",
    "\n",
    "```python\n",
    "\n",
    "from svoc.settings import get_settings\n",
    "from svoc.utils import read_data\n",
    "from svoc.supervised.enums import SupervisedModel\n",
    "from svoc.supervised.match import train_all_models\n",
    "from svoc.constants import DISTANCES\n",
    "\n",
    "settings = get_settings(\"./config/settings.yaml\")\n",
    "\n",
    "# Read Data\n",
    "df_input, df_benchmark = read_data(settings)\n",
    "\n",
    "# Models training\n",
    "models = train_all_models(\n",
    "    df_input=df_input,\n",
    "    input_cols_id_benchmark='sapcode',              ## <-- NEEDS TO BE SET\n",
    "    input_cols=settings.INPUT_COLUMNS_DICT,\n",
    "    df_benchmark=df_benchmark,\n",
    "    benchmark_cols=settings.BENCHMARK_COLUMNS_DICT,\n",
    "    distances=DISTANCES,\n",
    "    block_col=settings.BLOCK_COL,\n",
    "    window=5,\n",
    "    path_models=settings.SUPERVISED_MODELS_PATHS\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be31d4e2",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The script returns a dataframe with the following fields:\n",
    "\n",
    "- `ID_1`: Id of the records from the benchmark data;\n",
    "- `ID_2`: Id of the records from the input data;\n",
    "- `Rank`: An ordered ranking of the matches (from 1 to `N_MATCHES`) for each benchmark record, where rank 1 represents the first match;\n",
    "- `match_type`: Indicates whether the match was automatically selected through filters (`auto`) or selected through supervised probabilistic models (`supervised`);\n",
    "- `score`: the global similarity score summarizing the quality of the match according to the considered features or the used probabilistic model;\n",
    "- `*_score` (e.g. `OUTLET_NAME_score`): individual similarity scores for key attributes such as outlet name and address.\n",
    "- `*_method` (e.g. `OUTLET_NAME_method`): The string-matching techniques used to compute each field-level similarity score.  If the match is selected through supervised matching, the mathod is the Cosine similarity by deafault.\n",
    "- `ID_filter`: numeric identifier of the filtering step that selected the match (only if `match_type` is `auto`);\n",
    "- `model`: the probabilistic model used to select the match (only if `match_type` is `supervised`).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
